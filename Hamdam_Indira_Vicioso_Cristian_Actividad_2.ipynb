{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0148fc2-9305-42c4-81d1-819040125717",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# \uD83C\uDF0D Evidencia de Aprendizaje 2 (EA2) — Procesamiento de datos en una infraestructura cloud\n",
    "**Autores:**  \n",
    "- Indira Hamdam (Medellín, Antioquia, Colombia)\n",
    "- Cristian Vicioso (Santa Marta, Magdalena, Colombia)\n",
    "\n",
    "**Materia:** Big Data  \n",
    "**Institución:** I. U. Digital de Antioquia \n",
    "**Docente:** Andres Felipe Callejas Jaramillo  \n",
    "**Plataforma:** Databricks FRee Edition  \n",
    "**Dataset:** [Air Quality Data Set — Kaggle](https://www.kaggle.com/datasets/fedesoriano/air-quality-data-set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d3adbb6-5720-4343-949c-dd1761fd26d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83C\uDFAF Objetivo\n",
    "Desplegar y procesar un conjunto de datos reales en una infraestructura cloud (Databricks Free Edition), diseñando el esquema de almacenamiento, configurando el entorno, cargando datos desde Kaggle y validando consultas con Spark y SQL, bajo un enfoque Big Data aplicado al análisis de la calidad del aire en Colombia.\n",
    "\n",
    "## \uD83D\uDCE6 Dataset\n",
    "**Nombre:** Air Quality Data Set  \n",
    "**Autor:** Federico Soriano  \n",
    "**Fuente:** [https://www.kaggle.com/datasets/fedesoriano/air-quality-data-set](https://www.kaggle.com/datasets/fedesoriano/air-quality-data-set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d4e776e-2376-49ce-9e3e-3c96e857451c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83E\uDDE9 Diseño del esquema de almacenamiento — Dataset *Air Quality Data Set*\n",
    "\n",
    "El dataset seleccionado proviene de [Kaggle: Air Quality Data Set](https://www.kaggle.com/datasets/fedesoriano/air-quality-data-set), que contiene mediciones de calidad del aire registradas por sensores en diversas ciudades.  \n",
    "Este conjunto de datos es ideal para proyectos de **Big Data**, ya que permite analizar correlaciones entre contaminantes atmosféricos (PM2.5, NO₂, CO, etc.) y condiciones ambientales (temperatura, humedad).\n",
    "\n",
    "### \uD83C\uDFAF Objetivo del esquema\n",
    "Diseñar una estructura analítica que permita:\n",
    "- Almacenar mediciones de calidad del aire de forma estructurada.\n",
    "- Permitir consultas y análisis en Databricks utilizando Spark SQL.\n",
    "- Facilitar migraciones a arquitecturas más escalables de almacenamiento.\n",
    "\n",
    "---\n",
    "\n",
    "### \uD83E\uDDF1 Entidad principal: `AirQuality`\n",
    "| Campo | Tipo de dato | Nulabilidad | Descripción |\n",
    "|:------|:--------------|:-------------|:-------------|\n",
    "| `Date` | Date | No nulo | Fecha de la medición. |\n",
    "| `Time` | String | No nulo | Hora en formato HH:MM:SS. |\n",
    "| `CO_GT` | Double | Nulo | Concentración de monóxido de carbono (mg/m³). |\n",
    "| `NMHC_GT` | Double | Nulo | Hidrocarburos no metánicos (µg/m³). |\n",
    "| `C6H6_GT` | Double | Nulo | Nivel de benceno (µg/m³). |\n",
    "| `NOx_GT` | Double | Nulo | Concentración de óxidos de nitrógeno (ppb). |\n",
    "| `NO2_GT` | Double | Nulo | Concentración de dióxido de nitrógeno (µg/m³). |\n",
    "| `T` | Double | Nulo | Temperatura ambiente (°C). |\n",
    "| `RH` | Double | Nulo | Humedad relativa (%). |\n",
    "| `AH` | Double | Nulo | Humedad absoluta (g/m³). |\n",
    "\n",
    "**Clave primaria:** `(Date, Time)`  \n",
    "**Entidad complementaria sugerida:** `City` (para proyectos futuros que integren múltiples zonas o estaciones).\n",
    "\n",
    "---\n",
    "\n",
    "### \uD83E\uDDEE Representación visual del esquema (Mermaid)\n",
    "```mermaid\n",
    "erDiagram\n",
    "    AirQuality {\n",
    "        DATE Date\n",
    "        STRING Time\n",
    "        DOUBLE CO_GT\n",
    "        DOUBLE NMHC_GT\n",
    "        DOUBLE C6H6_GT\n",
    "        DOUBLE NOx_GT\n",
    "        DOUBLE NO2_GT\n",
    "        DOUBLE T\n",
    "        DOUBLE RH\n",
    "        DOUBLE AH\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71d2b25e-e160-43b8-a6ad-f7f0432e6148",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'struct<Date:date,Time:string,CO_GT:double,NMHC_GT:double,C6H6_GT:double,NOx_GT:double,NO2_GT:double,T:double,RH:double,AH:double>'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## \uD83D\uDCBB **Celda de código: Definición del esquema (StructType en PySpark)**\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, DoubleType, DateType\n",
    "\n",
    "schema_air = StructType([\n",
    "    StructField(\"Date\", DateType(), False),\n",
    "    StructField(\"Time\", StringType(), False),\n",
    "    StructField(\"CO_GT\", DoubleType(), True),\n",
    "    StructField(\"NMHC_GT\", DoubleType(), True),\n",
    "    StructField(\"C6H6_GT\", DoubleType(), True),\n",
    "    StructField(\"NOx_GT\", DoubleType(), True),\n",
    "    StructField(\"NO2_GT\", DoubleType(), True),\n",
    "    StructField(\"T\", DoubleType(), True),\n",
    "    StructField(\"RH\", DoubleType(), True),\n",
    "    StructField(\"AH\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "schema_air.simpleString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5226bf9d-5666-4dbb-a21d-eb247099095d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83E\uDDE0 Explicación del esquema\n",
    "\n",
    "El esquema fue diseñado para capturar de forma estructurada las variables ambientales de cada medición.  \n",
    "- Se emplearon tipos `DoubleType` para las mediciones numéricas y `DateType` / `StringType` para campos de fecha y hora.  \n",
    "- La clave primaria compuesta `(Date, Time)` permite identificar cada registro de manera única.  \n",
    "- Este diseño es compatible con **Spark SQL** y optimizado para análisis y almacenamiento distribuido en **Databricks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "92c3bae1-88fc-4795-a1b8-076f362a8114",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ⚙️ Configuración y evidencia de la infraestructura en Databricks CE\n",
    "\n",
    "Para esta práctica, se utiliza **Databricks Community Edition (Free Edition)** como entorno cloud para el procesamiento de datos con **Apache Spark**.\n",
    "\n",
    "### \uD83C\uDFD7️ Parámetros del clúster\n",
    "- **Nombre del clúster:** `AirQuality_BigData`\n",
    "- **Tipo de clúster:** Serverless (Community Edition)\n",
    "- **Runtime:** Databricks Runtime 14.x o superior (compatible con Spark 3.5+ / Python 3.11)\n",
    "- **Configuración:** 1 nodo (driver único), sin autoscaling disponible en CE\n",
    "- **Sistema de archivos:** DBFS (Databricks File System)\n",
    "\n",
    "El entorno se usa para ejecutar código PySpark y SQL en la nube, garantizando un espacio unificado para análisis, procesamiento y visualización de datos.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5a8b6d4-4e34-4f9c-a7e6-8d4a61fe0364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Versión de Spark: 4.0.0\n\uD83D\uDC0D Versión de Python: 3.11.10\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "\n",
    "# Versión de Spark y Python\n",
    "print(f\"✅ Versión de Spark: {spark.version}\")\n",
    "print(f\"\uD83D\uDC0D Versión de Python: {platform.python_version()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd3a4540-df85-488b-a101-4244542a0f1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Versión de Spark: 4.0.0\n\uD83D\uDC0D Versión de Python: 3.11.10\n"
     ]
    }
   ],
   "source": [
    "# Versión de Spark y Python (funciona en CE)\n",
    "import platform\n",
    "\n",
    "print(\"✅ Versión de Spark:\", spark.version)\n",
    "print(\"\uD83D\uDC0D Versión de Python:\", platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47557309-f2ac-4a0c-8d72-b906faa4eac9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚙️ Configuración resumida del entorno Spark (Free Edition):\n✅ Versión de Spark: 4.0.0\n⚠️ No se pudieron obtener todos los parámetros del clúster: [CONFIG_NOT_AVAILABLE] Configuration spark.app.name is not available. SQLSTATE: 42K0I\n\nJVM stacktrace:\norg.apache.spark.sql.AnalysisException\n\tat com.databricks.sql.connect.SparkConnectConfig$.assertConfigAllowedForRead(SparkConnectConfig.scala:297)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler$RuntimeConfigWrapper.get(SparkConnectConfigHandler.scala:113)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.transform(SparkConnectConfigHandler.scala:285)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handleGet$1(SparkConnectConfigHandler.scala:322)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handleGet$1$adapted(SparkConnectConfigHandler.scala:321)\n\tat scala.collection.IterableOnceOps.foreach(IterableOnce.scala:619)\n\tat scala.collection.IterableOnceOps.foreach$(IterableOnce.scala:617)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1306)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.handleGet(SparkConnectConfigHandler.scala:321)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$doHandle$1(SparkConnectConfigHandler.scala:237)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$doHandle$1$adapted(SparkConnectConfigHandler.scala:214)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:474)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$1(SessionHolder.scala:474)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:97)\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:121)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:115)\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:120)\n\tat org.apache.spark.sql.connect.service.SessionHolder.withSession(SessionHolder.scala:473)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.doHandle(SparkConnectConfigHandler.scala:214)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handle$1(SparkConnectConfigHandler.scala:205)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.$anonfun$handle$1$adapted(SparkConnectConfigHandler.scala:189)\n\tat com.databricks.spark.connect.logging.rpc.SparkConnectRpcMetricsCollectorUtils$.collectMetrics(SparkConnectRpcMetricsCollector.scala:265)\n\tat org.apache.spark.sql.connect.service.SparkConnectConfigHandler.handle(SparkConnectConfigHandler.scala:189)\n\tat org.apache.spark.sql.connect.service.SparkConnectService.config(SparkConnectService.scala:134)\n\tat org.apache.spark.connect.proto.SparkConnectServiceGrpc$MethodHandlers.invoke(SparkConnectServiceGrpc.java:874)\n\tat org.sparkproject.connect.io.grpc.stub.ServerCalls$UnaryServerCallHandler$UnaryServerCallListener.onHalfClose(ServerCalls.java:182)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.$anonfun$onHalfClose$1(AuthenticationInterceptor.scala:419)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat com.databricks.unity.HandleImpl.runWith(UCSHandle.scala:104)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$4(RequestContext.scala:366)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withAttributionContext(DatabricksSparkTracingHelper.scala:65)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpanFromRequest(DatabricksSparkTracingHelper.scala:92)\n\tat com.databricks.spark.util.DBRTracing$.withSpanFromRequest(DBRTracing.scala:43)\n\tat com.databricks.spark.connect.service.RequestContext.runWithSpanFromTags(RequestContext.scala:388)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$3(RequestContext.scala:366)\n\tat com.databricks.spark.connect.service.RequestContext$.com$databricks$spark$connect$service$RequestContext$$withLocalProperties(RequestContext.scala:584)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$2(RequestContext.scala:365)\n\tat com.databricks.logging.AttributionContextTracing.$anonfun$withAttributionContext$1(AttributionContextTracing.scala:49)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:328)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:324)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext(AttributionContextTracing.scala:47)\n\tat com.databricks.logging.AttributionContextTracing.withAttributionContext$(AttributionContextTracing.scala:44)\n\tat com.databricks.spark.util.PublicDBLogging.withAttributionContext(DatabricksSparkUsageLogger.scala:30)\n\tat com.databricks.spark.util.UniverseAttributionContextWrapper.withValue(AttributionContextUtils.scala:242)\n\tat com.databricks.spark.connect.service.RequestContext.$anonfun$runWith$1(RequestContext.scala:364)\n\tat com.databricks.spark.connect.service.RequestContext.withContext(RequestContext.scala:396)\n\tat com.databricks.spark.connect.service.RequestContext.runWith(RequestContext.scala:357)\n\tat com.databricks.spark.connect.service.AuthenticationInterceptor$AuthenticatedServerCallListener.onHalfClose(AuthenticationInterceptor.scala:419)\n\tat org.sparkproject.connect.io.grpc.PartialForwardingServerCallListener.onHalfClose(PartialForwardingServerCallListener.java:35)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:23)\n\tat org.sparkproject.connect.io.grpc.ForwardingServerCallListener$SimpleForwardingServerCallListener.onHalfClose(ForwardingServerCallListener.java:40)\n\tat org.sparkproject.connect.io.grpc.internal.ServerCallImpl$ServerStreamListenerImpl.halfClosed(ServerCallImpl.java:356)\n\tat org.sparkproject.connect.io.grpc.internal.ServerImpl$JumpToApplicationThreadServerStreamListener$1HalfClosed.runInContext(ServerImpl.java:861)\n\tat org.sparkproject.connect.io.grpc.internal.ContextRunnable.run(ContextRunnable.java:37)\n\tat org.sparkproject.connect.io.grpc.internal.SerializingExecutor.run(SerializingExecutor.java:133)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$6(SparkThreadLocalForwardingThreadPoolExecutor.scala:119)\n\tat com.databricks.sql.transaction.tahoe.mst.MSTThreadHelper$.runWithMstTxnId(MSTThreadHelper.scala:57)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$5(SparkThreadLocalForwardingThreadPoolExecutor.scala:118)\n\tat com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:117)\n\tat com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:51)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:116)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:93)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:162)\n\tat org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:165)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.lang.Thread.run(Thread.java:840)\n"
     ]
    }
   ],
   "source": [
    "print(\"⚙️ Configuración resumida del entorno Spark (Free Edition):\")\n",
    "\n",
    "# Spark version\n",
    "print(f\"✅ Versión de Spark: {spark.version}\")\n",
    "\n",
    "# Intentar obtener parámetros de configuración importantes\n",
    "try:\n",
    "    print(\"spark.app.name:\", spark.conf.get(\"spark.app.name\"))\n",
    "    print(\"spark.master:\", spark.conf.get(\"spark.master\"))\n",
    "    print(\"spark.sql.catalogImplementation:\", spark.conf.get(\"spark.sql.catalogImplementation\"))\n",
    "except Exception as e:\n",
    "    print(\"⚠️ No se pudieron obtener todos los parámetros del clúster:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2ebdb08-537f-43b9-be1a-e61e56cc6d55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### \uD83E\uDDE9 Interpretación de la configuración en Databricks Free Edition\n",
    "\n",
    "- Se confirmó la versión activa de **Spark** y **Python**.\n",
    "- En Databricks Free Edition, el entorno opera bajo un **catálogo Unity Catalog**, que gestiona el almacenamiento mediante **Volumes** y rutas dentro del Workspace.\n",
    "- No se permite escribir en el directorio público `/FileStore` (por razones de seguridad).\n",
    "- Los archivos se administran desde `/Workspace/Users/...` o `/Volumes/workspace/default/`, garantizando control de acceso por usuario.\n",
    "- Este entorno proporciona un espacio seguro y escalable para análisis con PySpark y SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "06a758aa-e021-4662-a55a-c8c53bd7bdb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## \uD83D\uDCCA Obtención del dataset y creación de tabla en Databricks\n",
    "\n",
    "En esta sección se ingiere el dataset **Air Quality Data Set** obtenido de Kaggle:  \n",
    "\uD83D\uDD17 [https://www.kaggle.com/datasets/fedesoriano/air-quality-data-set](https://www.kaggle.com/datasets/fedesoriano/air-quality-data-set)\n",
    "\n",
    "Se carga el archivo CSV previamente subido al entorno de **Databricks Free Edition**, ubicado en:  \n",
    "`/Workspace/Users/indira.hamdam@est.iudigital.edu.co/AirQuality.csv`\n",
    "\n",
    "El objetivo es leer los datos con **Spark**, aplicar el esquema diseñado, y crear una tabla persistente en el catálogo para posteriores consultas SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7997b10-3b0d-4233-9ccc-36f37d1879f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- Date: date (nullable = true)\n |-- Time: string (nullable = true)\n |-- CO(GT): string (nullable = true)\n |-- PT08.S1(CO): integer (nullable = true)\n |-- NMHC(GT): integer (nullable = true)\n |-- C6H6(GT): string (nullable = true)\n |-- PT08.S2(NMHC): integer (nullable = true)\n |-- NOx(GT): integer (nullable = true)\n |-- PT08.S3(NOx): integer (nullable = true)\n |-- NO2(GT): integer (nullable = true)\n |-- PT08.S4(NO2): integer (nullable = true)\n |-- PT08.S5(O3): integer (nullable = true)\n |-- T: string (nullable = true)\n |-- RH: string (nullable = true)\n |-- AH: string (nullable = true)\n |-- _c15: string (nullable = true)\n |-- _c16: string (nullable = true)\n\n+----------+--------+------+-----------+--------+--------+-------------+-------+------------+-------+------------+-----------+----+----+------+----+----+\n|      Date|    Time|CO(GT)|PT08.S1(CO)|NMHC(GT)|C6H6(GT)|PT08.S2(NMHC)|NOx(GT)|PT08.S3(NOx)|NO2(GT)|PT08.S4(NO2)|PT08.S5(O3)|   T|  RH|    AH|_c15|_c16|\n+----------+--------+------+-----------+--------+--------+-------------+-------+------------+-------+------------+-----------+----+----+------+----+----+\n|2004-03-10|18.00.00|   2,6|       1360|     150|    11,9|         1046|    166|        1056|    113|        1692|       1268|13,6|48,9|0,7578|NULL|NULL|\n|2004-03-10|19.00.00|     2|       1292|     112|     9,4|          955|    103|        1174|     92|        1559|        972|13,3|47,7|0,7255|NULL|NULL|\n|2004-03-10|20.00.00|   2,2|       1402|      88|     9,0|          939|    131|        1140|    114|        1555|       1074|11,9|54,0|0,7502|NULL|NULL|\n|2004-03-10|21.00.00|   2,2|       1376|      80|     9,2|          948|    172|        1092|    122|        1584|       1203|11,0|60,0|0,7867|NULL|NULL|\n|2004-03-10|22.00.00|   1,6|       1272|      51|     6,5|          836|    131|        1205|    116|        1490|       1110|11,2|59,6|0,7888|NULL|NULL|\n+----------+--------+------+-----------+--------+--------+-------------+-------+------------+-------+------------+-----------+----+----+------+----+----+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Ruta del archivo en el Workspace (ajustada para Databricks Free Edition)\n",
    "ruta = \"/Workspace/Users/indira.hamdam@est.iudigital.edu.co/AirQuality.csv\"\n",
    "\n",
    "# Lectura del CSV con Spark (usando inferSchema para detección automática)\n",
    "df_air = spark.read.csv(ruta, header=True, inferSchema=True, sep=\";\")\n",
    "\n",
    "# Mostrar esquema y primeras filas\n",
    "df_air.printSchema()\n",
    "df_air.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec2c6fd5-b9ce-4500-bc51-8e6bbc28956b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Nuevos nombres de columnas:\n['Date', 'Time', 'CO_GT', 'PT08_S1_CO', 'NMHC_GT', 'C6H6_GT', 'PT08_S2_NMHC', 'NOx_GT', 'PT08_S3_NOx', 'NO2_GT', 'PT08_S4_NO2', 'PT08_S5_O3', 'T', 'RH', 'AH', '_c15', '_c16']\n✅ Tabla 'air_quality' creada correctamente con nombres válidos.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Limpiar nombres de columnas (reemplazar caracteres no válidos)\n",
    "df_air_renamed = df_air.toDF(*[\n",
    "    c.replace(\"(\", \"_\")\n",
    "     .replace(\")\", \"\")\n",
    "     .replace(\".\", \"_\")\n",
    "     .replace(\"-\", \"_\")\n",
    "     .replace(\" \", \"_\")\n",
    "     .replace(\"/\", \"_\")\n",
    "     .replace(\"%\", \"pct\")\n",
    "     for c in df_air.columns\n",
    "])\n",
    "\n",
    "# Mostrar los nuevos nombres de columnas\n",
    "print(\"✅ Nuevos nombres de columnas:\")\n",
    "print(df_air_renamed.columns)\n",
    "\n",
    "# Guardar la tabla limpia en el catálogo\n",
    "df_air_renamed.write.mode(\"overwrite\").saveAsTable(\"air_quality\")\n",
    "\n",
    "print(\"✅ Tabla 'air_quality' creada correctamente con nombres válidos.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "225344a2-fb16-48fa-b5b7-034a65c3f8b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>col_name</th><th>data_type</th><th>comment</th></tr></thead><tbody><tr><td>Date_Time_COGT_PT08_S1CO_NMHCGT_C6H6GT_PT08_S2NMHC_NOxGT_PT08_S3NOx_NO2GT_PT08_S4NO2_PT08_S5O3_T_RH_AH__</td><td>string</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Date_Time_COGT_PT08_S1CO_NMHCGT_C6H6GT_PT08_S2NMHC_NOxGT_PT08_S3NOx_NO2GT_PT08_S4NO2_PT08_S5O3_T_RH_AH__",
         "string",
         null
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "col_name",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "data_type",
            "nullable": false,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "comment",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 128
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "col_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "data_type",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "comment",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "DESCRIBE TABLE air_quality_data;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91985ae2-2a88-4800-9929-c3d73a480a01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Propósito: Verificar la integridad estructural. Confirmamos que el esquema lógico diseñado se haya aplicado correctamente a los datos físicos.\n",
    "\n",
    "Interpretación del Resultado: Se valida que los nombres de las columnas estén normalizados.\n",
    "\n",
    "Se confirma que los tipos de datos sean correctos para realizar cálculos y no cadenas de texto, lo cual impediría operaciones matemáticas futuras.\n",
    "\n",
    "En SQL, se verifica que la tabla esté registrada en el metastore y sea accesible para consultas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7729d25-6db4-4eb1-8d6c-9da3393fb4ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>COUNT(*)</th></tr></thead><tbody><tr><td>9471</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         9471
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "COUNT(*)",
            "nullable": false,
            "type": "long"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 129
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "COUNT(*)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM air_quality_data;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cf68f51-d378-4052-9fa4-dc4eb953bcb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Date_Time_COGT_PT08_S1CO_NMHCGT_C6H6GT_PT08_S2NMHC_NOxGT_PT08_S3NOx_NO2GT_PT08_S4NO2_PT08_S5O3_T_RH_AH__</th></tr></thead><tbody><tr><td>10/03/2004;18.00.00;2</td></tr><tr><td>10/03/2004;19.00.00;2;1292;112;9</td></tr><tr><td>10/03/2004;20.00.00;2</td></tr><tr><td>10/03/2004;21.00.00;2</td></tr><tr><td>10/03/2004;22.00.00;1</td></tr><tr><td>10/03/2004;23.00.00;1</td></tr><tr><td>11/03/2004;00.00.00;1</td></tr><tr><td>11/03/2004;01.00.00;1;1136;31;3</td></tr><tr><td>11/03/2004;02.00.00;0</td></tr><tr><td>11/03/2004;03.00.00;0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "10/03/2004;18.00.00;2"
        ],
        [
         "10/03/2004;19.00.00;2;1292;112;9"
        ],
        [
         "10/03/2004;20.00.00;2"
        ],
        [
         "10/03/2004;21.00.00;2"
        ],
        [
         "10/03/2004;22.00.00;1"
        ],
        [
         "10/03/2004;23.00.00;1"
        ],
        [
         "11/03/2004;00.00.00;1"
        ],
        [
         "11/03/2004;01.00.00;1;1136;31;3"
        ],
        [
         "11/03/2004;02.00.00;0"
        ],
        [
         "11/03/2004;03.00.00;0"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "Date_Time_COGT_PT08_S1CO_NMHCGT_C6H6GT_PT08_S2NMHC_NOxGT_PT08_S3NOx_NO2GT_PT08_S4NO2_PT08_S5O3_T_RH_AH__",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 130
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Date_Time_COGT_PT08_S1CO_NMHCGT_C6H6GT_PT08_S2NMHC_NOxGT_PT08_S3NOx_NO2GT_PT08_S4NO2_PT08_S5O3_T_RH_AH__",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM air_quality LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "88501b94-8955-497f-b4ca-4161a8a79365",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Propósito: Validación de volumen y formato visual.\n",
    "\n",
    "Interpretación del Resultado: \n",
    "\n",
    "Count: Se compara contra la fuente original. Si el número coincide, la ingesta fue completa y no hubo pérdida de datos durante la carga.\n",
    "\n",
    "Limit/Muestra: Permite una inspección ocular rápida. Verifica que las columnas no estén desplazadas y que el parseo del separador decimal haya funcionado correctamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b5f009d1-7c97-4bf3-b013-5dfdb1843719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Análisis Comparativo: SQL vs. Spark (PySpark)\n",
    "En un ecosistema de Big Data como Databricks, ambos lenguajes conviven. La elección depende de quién lo usa (Analista vs. Ingeniero) y para qué lo usa (Reporting vs. Procesamiento Complejo).\n",
    "\n",
    "1. SQL (Spark SQL)\n",
    "Es el estándar de la industria para el análisis de datos. Su naturaleza es declarativa: le dices al motor qué datos quieres, y el optimizador (Catalyst en Spark) decide cómo obtenerlos.\n",
    "\n",
    "Fortalezas: Es imbatible en accesibilidad. Cualquier analista de negocio o usuario de BI puede interactuar con el Data Lake sin saber programar. Es ideal para validaciones rápidas, GROUP BY, y reportes finales.\n",
    "\n",
    "Debilidades: Se vuelve inmanejable en pipelines ETL muy complejos (cadenas de 500 líneas de código). Carece de bibliotecas nativas robustas para Machine Learning o manejo de estructuras de datos complejas (grafos, recursividad).\n",
    "\n",
    "2. Spark (PySpark)\n",
    "Es un enfoque imperativo (aunque con optimizaciones declarativas) que combina la potencia de Spark con la flexibilidad de Python.\n",
    "\n",
    "Fortalezas: Permite construir pipelines de ingeniería de datos robustos, modulares y testeables. Su integración con MLlib permite entrenar modelos de IA sobre los mismos DataFrames. Las UDFs (User Defined Functions) en Python permiten lógica de negocio muy específica que SQL no puede expresar.\n",
    "\n",
    "Debilidades: La curva de aprendizaje es mayor; requiere entender conceptos de computación distribuida (particiones, shuffles, lazy evaluation). Además, una mala configuración del código puede llevar a errores de memoria (OOM) más fácilmente que en SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0727a71-19a7-4a1a-a92a-1880b16b2422",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "### ⚔️ Cuadro Comparativo: SQL vs. Spark (PySpark)\n",
    "\n",
    "| Característica | \uD83C\uDFDB️ SQL (Spark SQL) | ⚡ Spark (PySpark) |\n",
    "| :--- | :--- | :--- |\n",
    "| **Enfoque** | **Declarativo:** Le dices *qué* quieres obtener. | **Imperativo:** Le dices *cómo* procesarlo paso a paso. |\n",
    "| **Curva de Aprendizaje** | **Baja.** Sintaxis universal, ideal para analistas y usuarios de negocio. | **Media-Alta.** Requiere saber Python y conceptos de computación distribuida. |\n",
    "| **Casos de Uso Ideales** | Validaciones rápidas, `GROUP BY`, reportes de BI y consultas ad-hoc. | Pipelines ETL complejos, limpieza de datos avanzada y Machine Learning. |\n",
    "| **Capacidad de ML** | **Limitada.** Depende de funciones básicas o integraciones externas. | **Nativa.** Integración directa con **MLlib** para entrenar modelos a escala. |\n",
    "| **Extensibilidad** | **Baja.** Crear funciones personalizadas es complejo. | **Alta.** Permite crear UDFs potentes en Python y Pandas. |\n",
    "| **Depuración (Debug)** | Difícil en cadenas largas de código (un solo bloque). | Fácil de depurar paso a paso al dividir en variables y métodos. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8e970d8b-b268-4147-9822-cc6c2a5a46f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "<h3 style=\"color:#1b3d6d\">\uD83D\uDE80 Comparativa Técnica: SQL vs Spark</h3>\n",
    "<table style=\"width:100%\">\n",
    "  <tr style=\"background-color:#f2f2f2\">\n",
    "    <th style=\"padding:10px\">Característica</th>\n",
    "    <th style=\"padding:10px\">SQL (Spark SQL)</th>\n",
    "    <th style=\"padding:10px\">Spark (PySpark)</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>Facilidad de Uso</strong></td>\n",
    "    <td>✅ Muy Alta (Estándar ANSI)</td>\n",
    "    <td>⚠️ Media (Requiere Python)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>Flexibilidad</strong></td>\n",
    "    <td>Limitada para lógica compleja</td>\n",
    "    <td>✅ Extrema (APIs ricas + UDFs)</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>Integración</strong></td>\n",
    "    <td>Excelente con herramientas BI (Tableau, PBI)</td>\n",
    "    <td>Excelente con librerías de IA/ML</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td><strong>Rendimiento</strong></td>\n",
    "    <td>Optimizado automáticamente (Catalyst)</td>\n",
    "    <td>Optimizado, pero requiere buenas prácticas</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8685867029745104,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Hamdam_Indira_Vicioso_Cristian_Actividad_2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}